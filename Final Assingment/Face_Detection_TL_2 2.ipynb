{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io"
      ],
      "metadata": {
        "id": "O6UYIxeF-HOq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVMkCDbS7K4a",
        "outputId": "53711274-477d-4ee4-bdbe-9bdf6e7bc82f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IsKCdLx56qW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3983e823-5c78-4df4-d234-da07509e8750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss', 'compile_metrics']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/model1.h5')\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Recompile with explicit metrics\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
        "\n",
        "# Check if metrics are now available\n",
        "print(model.metrics_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1FJHNqSWGKvP",
        "outputId": "9bf23e67-ab4b-4740-93ec-bb5a8c105d27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method TensorFlowTrainer.evaluate of <Sequential name=sequential_2, built=True>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>keras.src.backend.tensorflow.trainer.TensorFlowTrainer.evaluate</b><br/>def evaluate(x=None, y=None, batch_size=None, verbose=&#x27;auto&#x27;, sample_weight=None, steps=None, callbacks=None, return_dict=False, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py</a>Returns the loss value &amp; metrics values for the model in test mode.\n",
              "\n",
              "Computation is done in batches (see the `batch_size` arg.)\n",
              "\n",
              "Args:\n",
              "    x: Input data. It can be:\n",
              "        - A NumPy array (or array-like), or a list of arrays\n",
              "        (in case the model has multiple inputs).\n",
              "        - A backend-native tensor, or a list of tensors\n",
              "        (in case the model has multiple inputs).\n",
              "        - A dict mapping input names to the corresponding array/tensors,\n",
              "        if the model has named inputs.\n",
              "        - A `keras.utils.PyDataset` returning `(inputs, targets)` or\n",
              "        `(inputs, targets, sample_weights)`.\n",
              "        - A `tf.data.Dataset` yielding `(inputs, targets)` or\n",
              "        `(inputs, targets, sample_weights)`.\n",
              "        - A `torch.utils.data.DataLoader` yielding `(inputs, targets)`\n",
              "        or `(inputs, targets, sample_weights)`.\n",
              "        - A Python generator function yielding `(inputs, targets)` or\n",
              "        `(inputs, targets, sample_weights)`.\n",
              "    y: Target data. Like the input data `x`, it can be either NumPy\n",
              "        array(s) or backend-native tensor(s). If `x` is a\n",
              "        `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
              "        `torch.utils.data.DataLoader` or a Python generator function,\n",
              "        `y` should not be specified since targets will be obtained from\n",
              "        `x`.\n",
              "    batch_size: Integer or `None`.\n",
              "        Number of samples per batch of computation.\n",
              "        If unspecified, `batch_size` will default to 32.\n",
              "        Do not specify the `batch_size` if your input data `x` is a\n",
              "        `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
              "        `torch.utils.data.DataLoader` or Python generator function\n",
              "        since they generate batches.\n",
              "    verbose: `&quot;auto&quot;`, 0, 1, or 2. Verbosity mode.\n",
              "        0 = silent, 1 = progress bar, 2 = single line.\n",
              "        `&quot;auto&quot;` becomes 1 for most cases.\n",
              "        Note that the progress bar is not\n",
              "        particularly useful when logged to a file, so `verbose=2` is\n",
              "        recommended when not running interactively\n",
              "        (e.g. in a production environment). Defaults to `&quot;auto&quot;`.\n",
              "    sample_weight: Optional NumPy array or tensor of weights for\n",
              "        the training samples, used for weighting the loss function\n",
              "        (during training only). You can either pass a flat (1D)\n",
              "        NumPy array or tensor with the same length as the input samples\n",
              "        (1:1 mapping between weights and samples), or in the case of\n",
              "        temporal data, you can pass a 2D NumPy array or tensor with\n",
              "        shape `(samples, sequence_length)` to apply a different weight\n",
              "        to every timestep of every sample.\n",
              "        This argument is not supported when `x` is a\n",
              "        `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
              "        `torch.utils.data.DataLoader` or Python generator function.\n",
              "        Instead, provide `sample_weights` as the third element of `x`.\n",
              "        Note that sample weighting does not apply to metrics specified\n",
              "        via the `metrics` argument in `compile()`. To apply sample\n",
              "        weighting to your metrics, you can specify them via the\n",
              "        `weighted_metrics` in `compile()` instead.\n",
              "    steps: Integer or `None`.\n",
              "        Total number of steps (batches of samples) to draw before\n",
              "        declaring the evaluation round finished. If `steps` is `None`,\n",
              "        it will run until `x` is exhausted. In the case of an infinitely\n",
              "        repeating dataset, it will run indefinitely.\n",
              "    callbacks: List of `keras.callbacks.Callback` instances.\n",
              "        List of callbacks to apply during evaluation.\n",
              "    return_dict: If `True`, loss and metric results are returned as a\n",
              "        dict, with each key being the name of the metric.\n",
              "        If `False`, they are returned as a list.\n",
              "\n",
              "Returns:\n",
              "    Scalar test loss (if the model has a single output and no metrics)\n",
              "    or list of scalars (if the model has multiple outputs\n",
              "    and/or metrics). The attribute `model.metrics_names` will give you\n",
              "    the display labels for the scalar outputs.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 427);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "\n",
        "# Ensure to print the model input shape for verification\n",
        "print(\"Model input shape:\", model.input_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "vhyZwyZV7nqc",
        "outputId": "403c84f4-31d2-4799-f2f4-05ff75c3a7b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_128 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m163,968\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m516\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,468\u001b[0m (9.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,468</span> (9.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,484\u001b[0m (642.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,484</span> (642.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model input shape: (None, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing to match model's input size\n",
        "def preprocess_face(face, target_shape):\n",
        "    face_resized = cv2.resize(face, (target_shape[1], target_shape[2]))  # Resize to model input shape\n",
        "    face_normalized = face_resized / 255.0  # Normalize to [0, 1]\n",
        "    face_array = np.expand_dims(face_normalized, axis=0)  # Add batch dimension\n",
        "    return face_array\n"
      ],
      "metadata": {
        "id": "mEiZUZT7VT5T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle predictions\n",
        "def predict_label(face_array):\n",
        "    predictions = model.predict(face_array)\n",
        "    confidence = np.max(predictions)  # Get the maximum probability\n",
        "    label_index = np.argmax(predictions)  # Get the index of the highest probability class\n",
        "    return label_index, confidence\n"
      ],
      "metadata": {
        "id": "z2lst9JdVWvI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Camera** **ON**"
      ],
      "metadata": {
        "id": "9-5X0wi_AH1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL.Image\n",
        "import io\n",
        "import pickle\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "# Load the Haar Cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "\n",
        "pickle_in = open(\"/content/drive/MyDrive/label_encoder1.pkl\", \"rb\")\n",
        "label_encoder= pickle.load(pickle_in)  # Labels\n",
        "\n",
        "# Load class labels (names)\n",
        "with open('/content/drive/MyDrive/categories1.pkl', 'rb') as f:\n",
        "    class_labels = pickle.load(f)\n",
        "\n",
        "print(\"Loaded Class Labels:\", class_labels)\n",
        "print(\"Loaded Labels:\", class_labels)\n",
        "\n",
        "\n",
        "\n",
        "# Function to preprocess the face for the model\n",
        "def preprocess_face(face, target_shape):\n",
        "    face_resized = cv2.resize(face, (target_shape[1], target_shape[2]))\n",
        "    face_normalized = face_resized / 255.0\n",
        "    face_array = np.expand_dims(face_normalized, axis=0)  # Add batch dimension\n",
        "    if len(model.input_shape) == 2:  # If the model expects flattened input\n",
        "        face_array = face_array.reshape((1, -1))  # Flatten the array\n",
        "    return face_array\n",
        "\n",
        "# Function to handle predictions\n",
        "def predict_label(face_array):\n",
        "    predictions = model.predict(face_array, verbose=0)  # Set verbose=0 to suppress output\n",
        "    confidence = np.max(predictions)  # Get the maximum confidence\n",
        "    label_index = np.argmax(predictions)  # Get the index of the highest confidence class\n",
        "\n",
        "    return label_index, confidence\n",
        "\n",
        "\n",
        "# Function to convert JavaScript object to OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    return img\n",
        "\n",
        "# Function to convert OpenCV bounding box to base64 for overlay\n",
        "def bbox_to_bytes(bbox_array):\n",
        "    # Ensure bbox_array is in RGBA format\n",
        "    if len(bbox_array.shape) == 3 and bbox_array.shape[2] != 4:\n",
        "        bbox_array = cv2.cvtColor(bbox_array, cv2.COLOR_BGR2BGRA)  # Convert to BGRA if not already\n",
        "\n",
        "    # Convert NumPy array to PIL Image\n",
        "    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "\n",
        "    # Create an in-memory byte buffer\n",
        "    iobuf = io.BytesIO()\n",
        "\n",
        "    # Save the image as PNG to the byte buffer\n",
        "    bbox_PIL.save(iobuf, format='PNG')  # Use PNG to retain transparency\n",
        "\n",
        "    # Convert to base64 string and return\n",
        "    bbox_bytes = 'data:image/png;base64,{}'.format(str(b64encode(iobuf.getvalue()), 'utf-8'))\n",
        "    return bbox_bytes\n",
        "\n",
        "\n",
        "# JavaScript for video stream\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "        var video;\n",
        "        var div = null;\n",
        "        var stream;\n",
        "        var captureCanvas;\n",
        "        var imgElement;\n",
        "        var labelElement;\n",
        "\n",
        "        var pendingResolve = null;\n",
        "        var shutdown = false;\n",
        "\n",
        "        function removeDom() {\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            video.remove();\n",
        "            div.remove();\n",
        "            video = null;\n",
        "            div = null;\n",
        "            stream = null;\n",
        "            imgElement = null;\n",
        "            captureCanvas = null;\n",
        "            labelElement = null;\n",
        "        }\n",
        "\n",
        "        function onAnimationFrame() {\n",
        "            if (!shutdown) {\n",
        "                window.requestAnimationFrame(onAnimationFrame);\n",
        "            }\n",
        "            if (pendingResolve) {\n",
        "                var result = \"\";\n",
        "                if (!shutdown) {\n",
        "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "                    result = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "                }\n",
        "                var lp = pendingResolve;\n",
        "                pendingResolve = null;\n",
        "                lp(result);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function createDom() {\n",
        "            if (div !== null) {\n",
        "                return stream;\n",
        "            }\n",
        "\n",
        "            div = document.createElement('div');\n",
        "            div.style.border = '2px solid black';\n",
        "            div.style.padding = '3px';\n",
        "            div.style.width = '100%';\n",
        "            div.style.maxWidth = '600px';\n",
        "            document.body.appendChild(div);\n",
        "\n",
        "            const modelOut = document.createElement('div');\n",
        "            modelOut.innerHTML = \"Status:\";\n",
        "            labelElement = document.createElement('span');\n",
        "            labelElement.innerText = 'No data';\n",
        "            labelElement.style.fontWeight = 'bold';\n",
        "            modelOut.appendChild(labelElement);\n",
        "            div.appendChild(modelOut);\n",
        "\n",
        "            video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            video.width = div.clientWidth - 6;\n",
        "            video.setAttribute('playsinline', '');\n",
        "            video.onclick = () => { shutdown = true; };\n",
        "            stream = await navigator.mediaDevices.getUserMedia(\n",
        "                {video: { facingMode: \"environment\"}});\n",
        "            div.appendChild(video);\n",
        "\n",
        "            imgElement = document.createElement('img');\n",
        "            imgElement.style.position = 'absolute';\n",
        "            imgElement.style.zIndex = 1;\n",
        "            imgElement.onclick = () => { shutdown = true; };\n",
        "            div.appendChild(imgElement);\n",
        "\n",
        "            const instruction = document.createElement('div');\n",
        "            instruction.innerHTML =\n",
        "                '' +\n",
        "                'When finished, click \"S\" to stop this demo';\n",
        "            div.appendChild(instruction);\n",
        "            instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "            // Add key press event listener\n",
        "            window.addEventListener('keydown', function(e) {\n",
        "                if (e.key === 's' || e.key === 'S') {\n",
        "                    shutdown = true;  // Stop the video stream on 'S' key press\n",
        "                }\n",
        "            });\n",
        "\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            captureCanvas = document.createElement('canvas');\n",
        "            captureCanvas.width = 640;\n",
        "            captureCanvas.height = 480;\n",
        "            window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "            return stream;\n",
        "        }\n",
        "\n",
        "        async function stream_frame(label, imgData) {\n",
        "            if (shutdown) {\n",
        "                removeDom();\n",
        "                shutdown = false;\n",
        "                return '';\n",
        "            }\n",
        "\n",
        "            var preCreate = Date.now();\n",
        "            stream = await createDom();\n",
        "\n",
        "            var preShow = Date.now();\n",
        "            if (label != \"\") {\n",
        "                labelElement.innerHTML = label;\n",
        "            }\n",
        "\n",
        "            if (imgData != \"\") {\n",
        "                var videoRect = video.getClientRects()[0];\n",
        "                imgElement.style.top = videoRect.top + \"px\";\n",
        "                imgElement.style.left = videoRect.left + \"px\";\n",
        "                imgElement.style.width = videoRect.width + \"px\";\n",
        "                imgElement.style.height = videoRect.height + \"px\";\n",
        "                imgElement.src = imgData;\n",
        "            }\n",
        "\n",
        "            var preCapture = Date.now();\n",
        "            var result = await new Promise(function(resolve, reject) {\n",
        "                pendingResolve = resolve;\n",
        "            });\n",
        "            shutdown = false;\n",
        "\n",
        "            return {'create': preShow - preCreate,\n",
        "                    'show': preCapture - preShow,\n",
        "                    'capture': Date.now() - preCapture,\n",
        "                    'img': result};\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "    return data\n",
        "\n",
        "# Start video stream\n",
        "video_stream()\n",
        "label_html = 'Capturing...'\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convert JS response to OpenCV image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    bbox_array = np.zeros([480, 640, 3], dtype=np.uint8)  # Use 3 channels for BGR format\n",
        "\n",
        "    # Convert image to grayscale for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    # Draw bounding box and label on the image\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract and preprocess the detected face\n",
        "        face = img[y:y+h, x:x+w]\n",
        "        face_array = preprocess_face(face, target_shape=model.input_shape)\n",
        "\n",
        "        # Predict the class and confidence\n",
        "        label_index, confidence = predict_label(face_array)\n",
        "\n",
        "        # Determine the label based on confidence\n",
        "        if confidence > 0.9:  # Lowered confidence threshold for debugging\n",
        "            label = f\"{class_labels[label_index]} ({confidence*100:.2f}%)\"\n",
        "            color = (0, 255, 0)  # Green for recognized\n",
        "        else:\n",
        "            label = \"Unknown\"\n",
        "            color = (0, 0, 255)  # Red for unknown\n",
        "\n",
        "        # Draw the bounding box and label on the image\n",
        "        cv2.rectangle(bbox_array, (x, y), (x+w, y+h), color, 2)\n",
        "        cv2.putText(bbox_array, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
        "\n",
        "\n",
        "    # Convert bbox_array to RGB format if not already\n",
        "    bbox_array = cv2.cvtColor(bbox_array, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create an alpha channel for transparency\n",
        "    # Reduce the alpha value for partial transparency (e.g., 128 for 50% transparency)\n",
        "    alpha_channel = np.full(bbox_array.shape[:2], 128, dtype=np.uint8)\n",
        "\n",
        "    # Add the alpha channel to the bbox_array\n",
        "    bbox_array_with_alpha = np.dstack([bbox_array, alpha_channel])\n",
        "\n",
        "    # Convert the RGBA array to bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array_with_alpha)\n",
        "\n",
        "    # Send the overlay with the bounding box back to the frontend\n",
        "    bbox = bbox_bytes\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "LcXd3QtuPGcy",
        "outputId": "ff61e584-722c-4efd-ccc7-ae7b0290d1bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Class Labels: {0: 'Faysal', 1: 'Mohim', 2: 'Tanzil', 3: 'Tareq'}\n",
            "Loaded Labels: {0: 'Faysal', 1: 'Mohim', 2: 'Tanzil', 3: 'Tareq'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        var video;\n",
              "        var div = null;\n",
              "        var stream;\n",
              "        var captureCanvas;\n",
              "        var imgElement;\n",
              "        var labelElement;\n",
              "\n",
              "        var pendingResolve = null;\n",
              "        var shutdown = false;\n",
              "\n",
              "        function removeDom() {\n",
              "            stream.getVideoTracks()[0].stop();\n",
              "            video.remove();\n",
              "            div.remove();\n",
              "            video = null;\n",
              "            div = null;\n",
              "            stream = null;\n",
              "            imgElement = null;\n",
              "            captureCanvas = null;\n",
              "            labelElement = null;\n",
              "        }\n",
              "\n",
              "        function onAnimationFrame() {\n",
              "            if (!shutdown) {\n",
              "                window.requestAnimationFrame(onAnimationFrame);\n",
              "            }\n",
              "            if (pendingResolve) {\n",
              "                var result = \"\";\n",
              "                if (!shutdown) {\n",
              "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "                    result = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
              "                }\n",
              "                var lp = pendingResolve;\n",
              "                pendingResolve = null;\n",
              "                lp(result);\n",
              "            }\n",
              "        }\n",
              "\n",
              "        async function createDom() {\n",
              "            if (div !== null) {\n",
              "                return stream;\n",
              "            }\n",
              "\n",
              "            div = document.createElement('div');\n",
              "            div.style.border = '2px solid black';\n",
              "            div.style.padding = '3px';\n",
              "            div.style.width = '100%';\n",
              "            div.style.maxWidth = '600px';\n",
              "            document.body.appendChild(div);\n",
              "\n",
              "            const modelOut = document.createElement('div');\n",
              "            modelOut.innerHTML = \"Status:\";\n",
              "            labelElement = document.createElement('span');\n",
              "            labelElement.innerText = 'No data';\n",
              "            labelElement.style.fontWeight = 'bold';\n",
              "            modelOut.appendChild(labelElement);\n",
              "            div.appendChild(modelOut);\n",
              "\n",
              "            video = document.createElement('video');\n",
              "            video.style.display = 'block';\n",
              "            video.width = div.clientWidth - 6;\n",
              "            video.setAttribute('playsinline', '');\n",
              "            video.onclick = () => { shutdown = true; };\n",
              "            stream = await navigator.mediaDevices.getUserMedia(\n",
              "                {video: { facingMode: \"environment\"}});\n",
              "            div.appendChild(video);\n",
              "\n",
              "            imgElement = document.createElement('img');\n",
              "            imgElement.style.position = 'absolute';\n",
              "            imgElement.style.zIndex = 1;\n",
              "            imgElement.onclick = () => { shutdown = true; };\n",
              "            div.appendChild(imgElement);\n",
              "\n",
              "            const instruction = document.createElement('div');\n",
              "            instruction.innerHTML =\n",
              "                '' +\n",
              "                'When finished, click \"S\" to stop this demo';\n",
              "            div.appendChild(instruction);\n",
              "            instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "            // Add key press event listener\n",
              "            window.addEventListener('keydown', function(e) {\n",
              "                if (e.key === 's' || e.key === 'S') {\n",
              "                    shutdown = true;  // Stop the video stream on 'S' key press\n",
              "                }\n",
              "            });\n",
              "\n",
              "            video.srcObject = stream;\n",
              "            await video.play();\n",
              "\n",
              "            captureCanvas = document.createElement('canvas');\n",
              "            captureCanvas.width = 640;\n",
              "            captureCanvas.height = 480;\n",
              "            window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "            return stream;\n",
              "        }\n",
              "\n",
              "        async function stream_frame(label, imgData) {\n",
              "            if (shutdown) {\n",
              "                removeDom();\n",
              "                shutdown = false;\n",
              "                return '';\n",
              "            }\n",
              "\n",
              "            var preCreate = Date.now();\n",
              "            stream = await createDom();\n",
              "\n",
              "            var preShow = Date.now();\n",
              "            if (label != \"\") {\n",
              "                labelElement.innerHTML = label;\n",
              "            }\n",
              "\n",
              "            if (imgData != \"\") {\n",
              "                var videoRect = video.getClientRects()[0];\n",
              "                imgElement.style.top = videoRect.top + \"px\";\n",
              "                imgElement.style.left = videoRect.left + \"px\";\n",
              "                imgElement.style.width = videoRect.width + \"px\";\n",
              "                imgElement.style.height = videoRect.height + \"px\";\n",
              "                imgElement.src = imgData;\n",
              "            }\n",
              "\n",
              "            var preCapture = Date.now();\n",
              "            var result = await new Promise(function(resolve, reject) {\n",
              "                pendingResolve = resolve;\n",
              "            });\n",
              "            shutdown = false;\n",
              "\n",
              "            return {'create': preShow - preCreate,\n",
              "                    'show': preCapture - preShow,\n",
              "                    'capture': Date.now() - preCapture,\n",
              "                    'img': result};\n",
              "        }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8cc84a3505ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mjs_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjs_reply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8cc84a3505ec>\u001b[0m in \u001b[0;36mvideo_frame\u001b[0;34m(label, bbox)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream_frame(\"{}\", \"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}